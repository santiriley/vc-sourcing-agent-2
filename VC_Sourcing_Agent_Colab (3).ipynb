{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bee43e8f",
      "metadata": {
        "id": "bee43e8f"
      },
      "source": [
        "\n",
        "# VC Sourcing Agent — Colab Notebook (LatAm focus)\n",
        "\n",
        "This notebook pulls startup news (Google News RSS + LatAm tech feeds), tags **post‑revenue** and **female‑founder** signals, scores leads, and appends them to a **Google Sheet**.\n",
        "\n",
        "**Before running:**\n",
        "1. Create a Google Sheet (blank) in your Drive and copy its **Sheet ID** (between `/d/` and `/edit` in the URL).\n",
        "2. Open your **service account JSON** and copy the `client_email`. Share the Sheet with that email as **Editor**.\n",
        "3. In Step 2 below, upload your `service_account.json` into the Colab session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "127b14e7",
      "metadata": {
        "id": "127b14e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# IMPORTS AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import textwrap\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import tz\n",
        "from typing import Dict, List, Set, Optional, Any\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from gspread_dataframe import set_with_dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(\"service_account.json\"):\n",
        "    try:\n",
        "        from google.colab import files  # Colab only\n",
        "        uploaded = files.upload()\n",
        "        json_name = next(iter(uploaded))\n",
        "        os.replace(json_name, \"service_account.json\")\n",
        "        print(\"Uploaded and saved as service_account.json\")\n",
        "    except Exception:\n",
        "        print(\"CI mode: service_account.json must already exist; skipping upload.\")\n",
        "else:\n",
        "    print(\"service_account.json already present; skipping upload.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzCys9hByze-",
        "outputId": "81230122-ef5c-4b5f-9c29-56df82bf3ef6"
      },
      "id": "qzCys9hByze-",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "service_account.json already present; skipping upload.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3579baca",
      "metadata": {
        "id": "3579baca"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration settings for the VC Sourcing Agent\"\"\"\n",
        "\n",
        "    # Google Sheets Configuration\n",
        "    SHEET_ID = \"1AAH4gxlPVIfQMsNkLdKEBQOJXKmtxBM_8cXzc5hi7SM\"\n",
        "    SHEET_NAME = \"Leads\"\n",
        "    SCOPE = [\n",
        "        \"https://spreadsheets.google.com/feeds\",\n",
        "        \"https://www.googleapis.com/auth/drive\"\n",
        "    ]\n",
        "\n",
        "    # Timezone\n",
        "    TIMEZONE = \"America/Costa_Rica\"\n",
        "\n",
        "    # Data Collection Settings\n",
        "    TIME_WINDOW_DAYS = 14\n",
        "    MAX_ITEMS_PER_FEED = 60\n",
        "\n",
        "    # Geographic Coverage\n",
        "    COUNTRIES = [\n",
        "        \"Costa Rica\", \"Guatemala\", \"El Salvador\", \"Honduras\", \"Nicaragua\",\n",
        "        \"Panama\", \"Belize\", \"Colombia\", \"Venezuela\", \"Ecuador\", \"Peru\",\n",
        "        \"Bolivia\", \"Chile\", \"Argentina\", \"Uruguay\", \"Paraguay\", \"Brazil\"\n",
        "    ]\n",
        "\n",
        "    COUNTRY_ALIASES = {\n",
        "        \"Costa Rica\": [\"Costa Rica\", \"CR\"],\n",
        "        \"El Salvador\": [\"El Salvador\", \"SV\"],\n",
        "        \"Honduras\": [\"Honduras\", \"HN\"],\n",
        "        \"Nicaragua\": [\"Nicaragua\", \"NI\"],\n",
        "        \"Guatemala\": [\"Guatemala\", \"GT\"],\n",
        "        \"Panama\": [\"Panamá\", \"Panama\", \"PA\"],\n",
        "        \"Belize\": [\"Belize\"],\n",
        "        \"Colombia\": [\"Colombia\", \"CO\"],\n",
        "        \"Venezuela\": [\"Venezuela\", \"VE\"],\n",
        "        \"Ecuador\": [\"Ecuador\", \"EC\"],\n",
        "        \"Peru\": [\"Perú\", \"Peru\", \"PE\"],\n",
        "        \"Bolivia\": [\"Bolivia\", \"BO\"],\n",
        "        \"Chile\": [\"Chile\", \"CL\"],\n",
        "        \"Argentina\": [\"Argentina\", \"AR\"],\n",
        "        \"Uruguay\": [\"Uruguay\", \"UY\", \"ROU\"],\n",
        "        \"Paraguay\": [\"Paraguay\", \"PY\"],\n",
        "        \"Brazil\": [\"Brasil\", \"Brazil\", \"BR\"],\n",
        "    }\n",
        "\n",
        "    # Signal Detection Terms\n",
        "    SECTOR_BLACKLIST = [\n",
        "        \"fintech\", \"payments\", \"lending\", \"buy now pay later\",\n",
        "        \"wallet\", \"neobank\", \"crypto exchange\", \"remittance\"\n",
        "    ]\n",
        "\n",
        "    POST_REVENUE_TERMS = [\n",
        "        \"post-revenue\", \"revenue\", \"facturación\", \"ingresos\", \"ARR\", \"MRR\",\n",
        "        \"paying customers\", \"clientes de pago\", \"contratos\", \"invoices\",\n",
        "        \"compras recurrentes\"\n",
        "    ]\n",
        "\n",
        "    ENTERPRISE_SIGNALS = [\n",
        "        \"enterprise\", \"B2B\", \"contract\", \"pilot\", \"paid pilot\",\n",
        "        \"cliente corporativo\", \"empresa\"\n",
        "    ]\n",
        "\n",
        "    FEMALE_NAMES = [\n",
        "        \"ana\", \"maría\", \"maria\", \"camila\", \"daniela\", \"gabriela\", \"valentina\",\n",
        "        \"isabella\", \"sofia\", \"sofía\", \"fernanda\", \"luisa\", \"laura\", \"andrea\",\n",
        "        \"carla\", \"carolina\", \"paula\", \"juliana\", \"claudia\", \"patricia\",\n",
        "        \"mariana\", \"bianca\", \"bruna\", \"aline\", \"renata\", \"talita\", \"carol\",\n",
        "        \"alejandra\", \"ximena\", \"pauline\", \"ines\", \"inés\", \"beatriz\", \"raquel\",\n",
        "        \"cecilia\", \"catalina\", \"silvia\", \"verónica\", \"veronica\"\n",
        "    ]\n",
        "\n",
        "    # Feed URLs\n",
        "    LATAM_FEEDS = [\n",
        "        \"https://contxto.com/feed/\",\n",
        "        \"https://latamlist.com/feed/\"\n",
        "    ]\n",
        "\n",
        "# Allow GitHub Actions to override them:\n",
        "import os\n",
        "GOOGLE_SHEET_ID = os.getenv(\"GOOGLE_SHEET_ID\", GOOGLE_SHEET_ID)\n",
        "TZ = os.getenv(\"TZ\", TZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "30a260b2",
      "metadata": {
        "id": "30a260b2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# GOOGLE SHEETS INTEGRATION\n",
        "# ============================================================================\n",
        "\n",
        "class GoogleSheetsManager:\n",
        "    \"\"\"Handles Google Sheets authentication and operations\"\"\"\n",
        "\n",
        "    def __init__(self, service_account_path: str):\n",
        "        \"\"\"Initialize with service account credentials\"\"\"\n",
        "        self.service_account_path = service_account_path\n",
        "        self.client = None\n",
        "\n",
        "    def authenticate(self) -> gspread.Client:\n",
        "        \"\"\"Authenticate with Google Sheets API\"\"\"\n",
        "        if not self.client:\n",
        "            creds = ServiceAccountCredentials.from_json_keyfile_name(\n",
        "                self.service_account_path,\n",
        "                Config.SCOPE\n",
        "            )\n",
        "            self.client = gspread.authorize(creds)\n",
        "        return self.client\n",
        "\n",
        "    def open_sheet(self, sheet_id: str) -> gspread.Spreadsheet:\n",
        "        \"\"\"Open a Google Sheet by ID\"\"\"\n",
        "        client = self.authenticate()\n",
        "        return client.open_by_key(sheet_id)\n",
        "\n",
        "    def get_or_create_worksheet(\n",
        "        self,\n",
        "        sheet: gspread.Spreadsheet,\n",
        "        worksheet_name: str\n",
        "    ) -> gspread.Worksheet:\n",
        "        \"\"\"Get existing worksheet or create new one\"\"\"\n",
        "        try:\n",
        "            return sheet.worksheet(worksheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            return sheet.add_worksheet(\n",
        "                title=worksheet_name,\n",
        "                rows=1000,\n",
        "                cols=20\n",
        "            )\n",
        "\n",
        "    def read_existing_urls(self, worksheet: gspread.Worksheet) -> Set[str]:\n",
        "        \"\"\"Read existing URLs from worksheet to avoid duplicates\"\"\"\n",
        "        try:\n",
        "            records = worksheet.get_all_records()\n",
        "            return {r.get(\"URL\", \"\") for r in records if r.get(\"URL\")}\n",
        "        except Exception:\n",
        "            return set()\n",
        "\n",
        "    def append_dataframe(\n",
        "        self,\n",
        "        worksheet: gspread.Worksheet,\n",
        "        df: pd.DataFrame\n",
        "    ) -> None:\n",
        "        \"\"\"Append DataFrame to worksheet\"\"\"\n",
        "        all_values = worksheet.get_all_values()\n",
        "\n",
        "        if not all_values:\n",
        "            # Empty sheet - write with headers\n",
        "            set_with_dataframe(worksheet, df)\n",
        "        else:\n",
        "            # Append without headers\n",
        "            set_with_dataframe(\n",
        "                worksheet,\n",
        "                df,\n",
        "                row=len(all_values) + 1,\n",
        "                include_column_header=False\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "class DateTimeUtils:\n",
        "    \"\"\"Date and time utility functions\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_local_tz():\n",
        "        \"\"\"Get local timezone object\"\"\"\n",
        "        return tz.gettz(Config.TIMEZONE)\n",
        "\n",
        "    @staticmethod\n",
        "    def now() -> datetime:\n",
        "        \"\"\"Get current time in local timezone\"\"\"\n",
        "        return datetime.now(tz.tzutc()).astimezone(DateTimeUtils.get_local_tz())\n",
        "\n",
        "    @staticmethod\n",
        "    def is_within_window(dt: datetime) -> bool:\n",
        "        \"\"\"Check if datetime is within configured time window\"\"\"\n",
        "        age = DateTimeUtils.now() - dt\n",
        "        return age <= timedelta(days=Config.TIME_WINDOW_DAYS)\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_feed_date(entry) -> datetime:\n",
        "        \"\"\"Parse date from feed entry\"\"\"\n",
        "        for attr in (\"published_parsed\", \"updated_parsed\"):\n",
        "            if hasattr(entry, attr) and getattr(entry, attr):\n",
        "                parsed = getattr(entry, attr)\n",
        "                dt = datetime(*parsed[:6], tzinfo=tz.tzutc())\n",
        "                return dt.astimezone(DateTimeUtils.get_local_tz())\n",
        "        return DateTimeUtils.now()\n"
      ],
      "metadata": {
        "id": "07UP9h7OxF3U"
      },
      "id": "07UP9h7OxF3U",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TEXT ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "class TextAnalyzer:\n",
        "    \"\"\"Analyze text for signals and entities\"\"\"\n",
        "\n",
        "    # Regex for extracting names\n",
        "    NAME_PATTERN = re.compile(\n",
        "        r\"\\b([A-ZÁÉÍÓÚÑ][a-záéíóúñ]+)\\s([A-ZÁÉÍÓÚÑ][a-záéíóúñ]+)\\b\"\n",
        "    )\n",
        "\n",
        "    @staticmethod\n",
        "    def find_country(text: str) -> str:\n",
        "        \"\"\"Find country mentioned in text\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for country, aliases in Config.COUNTRY_ALIASES.items():\n",
        "            for alias in aliases:\n",
        "                if alias.lower() in text_lower:\n",
        "                    return country\n",
        "        return \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def contains_terms(text: str, terms: List[str]) -> bool:\n",
        "        \"\"\"Check if text contains any of the specified terms\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        return any(term.lower() in text_lower for term in terms)\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_female_founder(text: str) -> bool:\n",
        "        \"\"\"Detect if text mentions a female founder\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        founder_indicators = [\n",
        "            \"founded by\", \"co-founded by\", \"cofundada por\", \"fundada por\",\n",
        "            \"fundado por\", \"cofounder\", \"co-founder\", \"fundadora\",\n",
        "            \"fundador\", \"CEO\", \"CTO\"\n",
        "        ]\n",
        "\n",
        "        if not any(indicator in text_lower for indicator in founder_indicators):\n",
        "            return False\n",
        "\n",
        "        # Extract names and check against female names list\n",
        "        names = TextAnalyzer.NAME_PATTERN.findall(text)\n",
        "        for first_name, _ in names:\n",
        "            if first_name.lower() in Config.FEMALE_NAMES:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_company_name(text: str) -> str:\n",
        "        \"\"\"Attempt to extract company name from text\"\"\"\n",
        "        names = TextAnalyzer.NAME_PATTERN.findall(text)\n",
        "        if not names:\n",
        "            return \"\"\n",
        "\n",
        "        # Find most frequent name pair\n",
        "        name_pairs = [\" \".join(n) for n in names]\n",
        "        if name_pairs:\n",
        "            return max(set(name_pairs), key=name_pairs.count)\n",
        "        return \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_score(country: str, text: str) -> int:\n",
        "        \"\"\"Calculate lead score based on signals\"\"\"\n",
        "        score = 0\n",
        "\n",
        "        # Geographic relevance\n",
        "        if country:\n",
        "            score += 3\n",
        "\n",
        "        # Revenue signals\n",
        "        if TextAnalyzer.contains_terms(text, Config.POST_REVENUE_TERMS):\n",
        "            score += 3\n",
        "\n",
        "        # Female founder bonus\n",
        "        if TextAnalyzer.detect_female_founder(text):\n",
        "            score += 2\n",
        "\n",
        "        # Enterprise signals\n",
        "        if TextAnalyzer.contains_terms(text, Config.ENTERPRISE_SIGNALS):\n",
        "            score += 1\n",
        "\n",
        "        # Sector penalty\n",
        "        if TextAnalyzer.contains_terms(text, Config.SECTOR_BLACKLIST):\n",
        "            score -= 2\n",
        "\n",
        "        return max(0, min(10, score))"
      ],
      "metadata": {
        "id": "A40YW181xPmd"
      },
      "id": "A40YW181xPmd",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b2180e89",
      "metadata": {
        "id": "b2180e89"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FEED PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class FeedProcessor:\n",
        "    \"\"\"Process RSS feeds and extract startup information\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build_google_news_urls(country: str) -> List[str]:\n",
        "        \"\"\"Build Google News RSS URLs for a country\"\"\"\n",
        "        query = (\n",
        "            '(startup OR raised OR funding OR seed OR \"Series A\" '\n",
        "            'OR clients OR customers OR revenue OR facturación OR ingresos)'\n",
        "        )\n",
        "        full_query = f'{query} {country}'\n",
        "        encoded_query = quote_plus(full_query)\n",
        "\n",
        "        return [\n",
        "            f'https://news.google.com/rss/search?q={encoded_query}&hl=es-419&gl=LA&ceid=LA:es-419',\n",
        "            f'https://news.google.com/rss/search?q={encoded_query}&hl=en&gl=US&ceid=US:en'\n",
        "        ]\n",
        "\n",
        "    @staticmethod\n",
        "    def fetch_feed_items() -> List[Dict[str, Any]]:\n",
        "        \"\"\"Fetch all feed items from configured sources\"\"\"\n",
        "        items = []\n",
        "\n",
        "        # Process Google News feeds for each country\n",
        "        for country in Config.COUNTRIES:\n",
        "            urls = FeedProcessor.build_google_news_urls(country)\n",
        "\n",
        "            for url in urls:\n",
        "                try:\n",
        "                    feed = feedparser.parse(url)\n",
        "                    items.extend(\n",
        "                        FeedProcessor._process_feed_entries(\n",
        "                            feed.entries[:Config.MAX_ITEMS_PER_FEED],\n",
        "                            \"GoogleNews\",\n",
        "                            country\n",
        "                        )\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing feed {url}: {e}\")\n",
        "\n",
        "        # Process LatAm-specific feeds\n",
        "        for url in Config.LATAM_FEEDS:\n",
        "            try:\n",
        "                feed = feedparser.parse(url)\n",
        "                items.extend(\n",
        "                    FeedProcessor._process_feed_entries(\n",
        "                        feed.entries[:Config.MAX_ITEMS_PER_FEED],\n",
        "                        url,\n",
        "                        None\n",
        "                    )\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing feed {url}: {e}\")\n",
        "\n",
        "        return items\n",
        "\n",
        "    @staticmethod\n",
        "    def _process_feed_entries(\n",
        "        entries: List,\n",
        "        source: str,\n",
        "        default_country: Optional[str]\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"Process individual feed entries\"\"\"\n",
        "        items = []\n",
        "\n",
        "        for entry in entries:\n",
        "            dt = DateTimeUtils.parse_feed_date(entry)\n",
        "\n",
        "            if not DateTimeUtils.is_within_window(dt):\n",
        "                continue\n",
        "\n",
        "            title = entry.get(\"title\", \"\")\n",
        "            summary = entry.get(\"summary\", \"\")\n",
        "            link = entry.get(\"link\", \"\")\n",
        "\n",
        "            # Determine country\n",
        "            full_text = f\"{title}\\n{summary}\"\n",
        "            country = TextAnalyzer.find_country(full_text) or default_country or \"\"\n",
        "\n",
        "            items.append({\n",
        "                \"title\": title,\n",
        "                \"summary\": summary,\n",
        "                \"url\": link,\n",
        "                \"published\": dt,\n",
        "                \"source\": source,\n",
        "                \"country_guess\": country\n",
        "            })\n",
        "\n",
        "        return items\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA TRANSFORMATION\n",
        "# ============================================================================\n",
        "\n",
        "class DataTransformer:\n",
        "    \"\"\"Transform raw feed items into structured lead data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_html(text: str) -> str:\n",
        "        \"\"\"Remove HTML tags from text\"\"\"\n",
        "        return re.sub(r\"<[^<]+?>\", \"\", text)\n",
        "\n",
        "    @staticmethod\n",
        "    def truncate_text(text: str, max_length: int = 220) -> str:\n",
        "        \"\"\"Truncate text with ellipsis if needed\"\"\"\n",
        "        if len(text) > max_length:\n",
        "            return text[:max_length - 3] + \"…\"\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def transform_items(items: List[Dict]) -> pd.DataFrame:\n",
        "        \"\"\"Transform feed items into DataFrame with scoring\"\"\"\n",
        "        rows = []\n",
        "\n",
        "        for item in items:\n",
        "            title = item[\"title\"]\n",
        "            summary = DataTransformer.clean_html(item.get(\"summary\", \"\"))\n",
        "            url = item[\"url\"]\n",
        "            country = item[\"country_guess\"]\n",
        "\n",
        "            full_text = f\"{title}. {summary}\"\n",
        "\n",
        "            # Detect signals\n",
        "            signals = []\n",
        "            if TextAnalyzer.contains_terms(full_text, Config.POST_REVENUE_TERMS):\n",
        "                signals.append(\"post-revenue\")\n",
        "            if TextAnalyzer.detect_female_founder(full_text):\n",
        "                signals.append(\"female-founder\")\n",
        "            if TextAnalyzer.contains_terms(full_text, Config.ENTERPRISE_SIGNALS):\n",
        "                signals.append(\"enterprise\")\n",
        "            if TextAnalyzer.contains_terms(full_text, Config.SECTOR_BLACKLIST):\n",
        "                signals.append(\"fintech-ish\")\n",
        "\n",
        "            # Extract company name\n",
        "            company = (\n",
        "                TextAnalyzer.extract_company_name(title) or\n",
        "                TextAnalyzer.extract_company_name(summary) or\n",
        "                \"\"\n",
        "            )\n",
        "\n",
        "            rows.append({\n",
        "                \"DateFound\": DateTimeUtils.now().strftime(\"%Y-%m-%d\"),\n",
        "                \"Company\": company,\n",
        "                \"URL\": url,\n",
        "                \"Country\": country,\n",
        "                \"Title\": title,\n",
        "                \"Snippet\": DataTransformer.truncate_text(summary),\n",
        "                \"Signals\": \", \".join(signals),\n",
        "                \"Score\": TextAnalyzer.calculate_score(country, full_text),\n",
        "                \"Source\": item[\"source\"],\n",
        "                \"Published\": item[\"published\"].strftime(\"%Y-%m-%d %H:%M\")\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "\n",
        "        if not df.empty:\n",
        "            df = df.sort_values(\n",
        "                [\"Score\", \"Published\"],\n",
        "                ascending=[False, False]\n",
        "            ).reset_index(drop=True)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "IUEGsTV8xbUM"
      },
      "id": "IUEGsTV8xbUM",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "class VCSourcingPipeline:\n",
        "    \"\"\"Main pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self, service_account_path: str = \"service_account.json\"):\n",
        "        \"\"\"Initialize pipeline with service account\"\"\"\n",
        "        self.sheets_manager = GoogleSheetsManager(service_account_path)\n",
        "\n",
        "    def run(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Run the complete sourcing pipeline\"\"\"\n",
        "        print(\"⏳ Starting VC Sourcing Pipeline...\")\n",
        "\n",
        "        # Step 1: Collect feed items\n",
        "        print(\"📡 Collecting feed items...\")\n",
        "        items = FeedProcessor.fetch_feed_items()\n",
        "        print(f\"✓ Collected {len(items)} raw items\")\n",
        "\n",
        "        if not items:\n",
        "            print(\"⚠️ No items found within time window\")\n",
        "            return None\n",
        "\n",
        "        # Step 2: Transform data\n",
        "        print(\"🔄 Transforming data...\")\n",
        "        df = DataTransformer.transform_items(items)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"⚠️ No candidate leads after transformation\")\n",
        "            return None\n",
        "\n",
        "        print(f\"✓ Found {len(df)} candidate leads\")\n",
        "\n",
        "        # Step 3: Update Google Sheets\n",
        "        print(\"📊 Updating Google Sheets...\")\n",
        "        try:\n",
        "            sheet = self.sheets_manager.open_sheet(Config.SHEET_ID)\n",
        "            worksheet = self.sheets_manager.get_or_create_worksheet(\n",
        "                sheet,\n",
        "                Config.SHEET_NAME\n",
        "            )\n",
        "\n",
        "            # Check for duplicates\n",
        "            existing_urls = self.sheets_manager.read_existing_urls(worksheet)\n",
        "            new_df = df[~df[\"URL\"].isin(existing_urls)].copy()\n",
        "\n",
        "            if new_df.empty:\n",
        "                print(\"ℹ️ No new leads to add (all URLs already exist)\")\n",
        "                return df\n",
        "\n",
        "            # Append new leads\n",
        "            self.sheets_manager.append_dataframe(worksheet, new_df)\n",
        "\n",
        "            print(f\"✅ Added {len(new_df)} new leads to '{Config.SHEET_NAME}'\")\n",
        "            print(f\"🔗 Sheet: https://docs.google.com/spreadsheets/d/{Config.SHEET_ID}/edit\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error updating Google Sheets: {e}\")\n",
        "            return df\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "8mKiSidTxdHl"
      },
      "id": "8mKiSidTxdHl",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For Google Colab usage:\n",
        "    # 1. Upload your service_account.json file\n",
        "    # 2. Set your Google Sheet ID in Config.SHEET_ID\n",
        "    # 3. Run the pipeline\n",
        "\n",
        "    pipeline = VCSourcingPipeline(\"service_account.json\")\n",
        "    results = pipeline.run()\n",
        "\n",
        "    if results is not None:\n",
        "        print(\"\\n📋 Top 10 Leads:\")\n",
        "        print(results.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjfxlQFsxkXJ",
        "outputId": "dc7f44e1-17a7-4398-994a-393cb676235e"
      },
      "id": "WjfxlQFsxkXJ",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Starting VC Sourcing Pipeline...\n",
            "📡 Collecting feed items...\n",
            "✓ Collected 151 raw items\n",
            "🔄 Transforming data...\n",
            "✓ Found 151 candidate leads\n",
            "📊 Updating Google Sheets...\n",
            "✅ Added 151 new leads to 'Leads'\n",
            "🔗 Sheet: https://docs.google.com/spreadsheets/d/1AAH4gxlPVIfQMsNkLdKEBQOJXKmtxBM_8cXzc5hi7SM/edit\n",
            "\n",
            "📋 Top 10 Leads:\n",
            "    DateFound                  Company  \\\n",
            "0  2025-08-29  Facturación Electrónica   \n",
            "1  2025-08-29          Valora Analitik   \n",
            "2  2025-08-29          Suizo Argentina   \n",
            "3  2025-08-29             El Argentino   \n",
            "4  2025-08-29             Hive Digital   \n",
            "5  2025-08-29                            \n",
            "6  2025-08-29              El Salvador   \n",
            "7  2025-08-29                            \n",
            "8  2025-08-29                Enter Top   \n",
            "9  2025-08-29      Gaming Intelligence   \n",
            "\n",
            "                                                 URL      Country  \\\n",
            "0  https://news.google.com/rss/articles/CBMi4wFBV...  El Salvador   \n",
            "1  https://news.google.com/rss/articles/CBMiwwFBV...  El Salvador   \n",
            "2  https://news.google.com/rss/articles/CBMivgFBV...     Honduras   \n",
            "3  https://news.google.com/rss/articles/CBMiywFBV...  El Salvador   \n",
            "4  https://news.google.com/rss/articles/CBMiygFBV...     Honduras   \n",
            "5  https://news.google.com/rss/articles/CBMi4wFBV...   Costa Rica   \n",
            "6  https://news.google.com/rss/articles/CBMisgFBV...   Costa Rica   \n",
            "7  https://news.google.com/rss/articles/CBMingJBV...     Honduras   \n",
            "8  https://news.google.com/rss/articles/CBMi5AFBV...     Colombia   \n",
            "9  https://news.google.com/rss/articles/CBMiuwFBV...  El Salvador   \n",
            "\n",
            "                                               Title  \\\n",
            "0  Facturación Electrónica en El Salvador: Guía s...   \n",
            "1  Empresas en Colombia podrán ser sancionadas po...   \n",
            "2  La Provincia confirmó que Suizo Argentina es p...   \n",
            "3  Celulosa Argentina reportó desplome de ingreso...   \n",
            "4  Hive Digital Technologies Sets New Records in ...   \n",
            "5  El algodón argentino está liderando la vanguar...   \n",
            "6  Ingresos tributarios de El Salvador crecen 8.1...   \n",
            "7  Canal de Panamá prevé acercamiento al mercado ...   \n",
            "8  Nexus International Leverages Brazil’s Project...   \n",
            "9  Brazil gambling market generates revenue of R$...   \n",
            "\n",
            "                                             Snippet  \\\n",
            "0  Facturación Electrónica en El Salvador: Guía s...   \n",
            "1  Empresas en Colombia podrán ser sancionadas po...   \n",
            "2  La Provincia confirmó que Suizo Argentina es p...   \n",
            "3  Celulosa Argentina reportó desplome de ingreso...   \n",
            "4  Hive Digital Technologies Sets New Records in ...   \n",
            "5  El algodón argentino está liderando la vanguar...   \n",
            "6  Ingresos tributarios de El Salvador crecen 8.1...   \n",
            "7  Canal de Panamá prevé acercamiento al mercado ...   \n",
            "8  Nexus International Leverages Brazil’s Project...   \n",
            "9  Brazil gambling market generates revenue of R$...   \n",
            "\n",
            "                    Signals  Score      Source         Published  \n",
            "0  post-revenue, enterprise      7  GoogleNews  2025-08-27 08:38  \n",
            "1  post-revenue, enterprise      7  GoogleNews  2025-08-21 16:15  \n",
            "2              post-revenue      6  GoogleNews  2025-08-28 14:38  \n",
            "3              post-revenue      6  GoogleNews  2025-08-28 13:41  \n",
            "4              post-revenue      6  GoogleNews  2025-08-28 13:08  \n",
            "5              post-revenue      6  GoogleNews  2025-08-28 11:07  \n",
            "6              post-revenue      6  GoogleNews  2025-08-28 10:58  \n",
            "7              post-revenue      6  GoogleNews  2025-08-28 06:42  \n",
            "8              post-revenue      6  GoogleNews  2025-08-28 04:07  \n",
            "9              post-revenue      6  GoogleNews  2025-08-28 01:15  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9d29c9",
      "metadata": {
        "id": "8d9d29c9"
      },
      "source": [
        "\n",
        "### Optional: keep the key handy\n",
        "If you don't want to re-upload the JSON each session, you can mount Drive and keep `service_account.json` there.\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Then move/copy your JSON into /content/drive/MyDrive/ and change the path in authorize_from_json()\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
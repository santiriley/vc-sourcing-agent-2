{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bee43e8f",
      "metadata": {
        "id": "bee43e8f"
      },
      "source": [
        "\n",
        "# VC Sourcing Agent ‚Äî Colab Notebook (LatAm focus)\n",
        "\n",
        "This notebook pulls startup news (Google News RSS + LatAm tech feeds), tags **post‚Äërevenue** and **female‚Äëfounder** signals, scores leads, and appends them to a **Google Sheet**.\n",
        "\n",
        "**Before running:**\n",
        "1. Create a Google Sheet (blank) in your Drive and copy its **Sheet ID** (between `/d/` and `/edit` in the URL).\n",
        "2. Open your **service account JSON** and copy the `client_email`. Share the Sheet with that email as **Editor**.\n",
        "3. In Step 2 below, upload your `service_account.json` into the Colab session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "127b14e7",
      "metadata": {
        "id": "127b14e7"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# IMPORTS AND SETUP\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import textwrap\n",
        "from datetime import datetime, timedelta\n",
        "from dateutil import tz\n",
        "from typing import Dict, List, Set, Optional, Any\n",
        "from urllib.parse import quote_plus\n",
        "\n",
        "import pandas as pd\n",
        "import feedparser\n",
        "import gspread\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from gspread_dataframe import set_with_dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "if not os.path.exists(\"service_account.json\"):\n",
        "    try:\n",
        "        from google.colab import files  # Colab only\n",
        "        uploaded = files.upload()\n",
        "        json_name = next(iter(uploaded))\n",
        "        os.replace(json_name, \"service_account.json\")\n",
        "        print(\"Uploaded and saved as service_account.json\")\n",
        "    except Exception:\n",
        "        print(\"CI mode: service_account.json must already exist; skipping upload.\")\n",
        "else:\n",
        "    print(\"service_account.json already present; skipping upload.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzCys9hByze-",
        "outputId": "81230122-ef5c-4b5f-9c29-56df82bf3ef6"
      },
      "id": "qzCys9hByze-",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "service_account.json already present; skipping upload.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "3579baca",
      "metadata": {
        "id": "3579baca"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration settings for the VC Sourcing Agent\"\"\"\n",
        "\n",
        "    # Google Sheets Configuration\n",
        "    SHEET_ID = \"1AAH4gxlPVIfQMsNkLdKEBQOJXKmtxBM_8cXzc5hi7SM\"\n",
        "    SHEET_NAME = \"Leads\"\n",
        "    SCOPE = [\n",
        "        \"https://spreadsheets.google.com/feeds\",\n",
        "        \"https://www.googleapis.com/auth/drive\"\n",
        "    ]\n",
        "\n",
        "    # Timezone\n",
        "    TIMEZONE = \"America/Costa_Rica\"\n",
        "\n",
        "    # Data Collection Settings\n",
        "    TIME_WINDOW_DAYS = 14\n",
        "    MAX_ITEMS_PER_FEED = 60\n",
        "\n",
        "    # Geographic Coverage\n",
        "    COUNTRIES = [\n",
        "        \"Costa Rica\", \"Guatemala\", \"El Salvador\", \"Honduras\", \"Nicaragua\",\n",
        "        \"Panama\", \"Belize\", \"Colombia\", \"Venezuela\", \"Ecuador\", \"Peru\",\n",
        "        \"Bolivia\", \"Chile\", \"Argentina\", \"Uruguay\", \"Paraguay\", \"Brazil\"\n",
        "    ]\n",
        "\n",
        "    COUNTRY_ALIASES = {\n",
        "        \"Costa Rica\": [\"Costa Rica\", \"CR\"],\n",
        "        \"El Salvador\": [\"El Salvador\", \"SV\"],\n",
        "        \"Honduras\": [\"Honduras\", \"HN\"],\n",
        "        \"Nicaragua\": [\"Nicaragua\", \"NI\"],\n",
        "        \"Guatemala\": [\"Guatemala\", \"GT\"],\n",
        "        \"Panama\": [\"Panam√°\", \"Panama\", \"PA\"],\n",
        "        \"Belize\": [\"Belize\"],\n",
        "        \"Colombia\": [\"Colombia\", \"CO\"],\n",
        "        \"Venezuela\": [\"Venezuela\", \"VE\"],\n",
        "        \"Ecuador\": [\"Ecuador\", \"EC\"],\n",
        "        \"Peru\": [\"Per√∫\", \"Peru\", \"PE\"],\n",
        "        \"Bolivia\": [\"Bolivia\", \"BO\"],\n",
        "        \"Chile\": [\"Chile\", \"CL\"],\n",
        "        \"Argentina\": [\"Argentina\", \"AR\"],\n",
        "        \"Uruguay\": [\"Uruguay\", \"UY\", \"ROU\"],\n",
        "        \"Paraguay\": [\"Paraguay\", \"PY\"],\n",
        "        \"Brazil\": [\"Brasil\", \"Brazil\", \"BR\"],\n",
        "    }\n",
        "\n",
        "    # Signal Detection Terms\n",
        "    SECTOR_BLACKLIST = [\n",
        "        \"fintech\", \"payments\", \"lending\", \"buy now pay later\",\n",
        "        \"wallet\", \"neobank\", \"crypto exchange\", \"remittance\"\n",
        "    ]\n",
        "\n",
        "    POST_REVENUE_TERMS = [\n",
        "        \"post-revenue\", \"revenue\", \"facturaci√≥n\", \"ingresos\", \"ARR\", \"MRR\",\n",
        "        \"paying customers\", \"clientes de pago\", \"contratos\", \"invoices\",\n",
        "        \"compras recurrentes\"\n",
        "    ]\n",
        "\n",
        "    ENTERPRISE_SIGNALS = [\n",
        "        \"enterprise\", \"B2B\", \"contract\", \"pilot\", \"paid pilot\",\n",
        "        \"cliente corporativo\", \"empresa\"\n",
        "    ]\n",
        "\n",
        "    FEMALE_NAMES = [\n",
        "        \"ana\", \"mar√≠a\", \"maria\", \"camila\", \"daniela\", \"gabriela\", \"valentina\",\n",
        "        \"isabella\", \"sofia\", \"sof√≠a\", \"fernanda\", \"luisa\", \"laura\", \"andrea\",\n",
        "        \"carla\", \"carolina\", \"paula\", \"juliana\", \"claudia\", \"patricia\",\n",
        "        \"mariana\", \"bianca\", \"bruna\", \"aline\", \"renata\", \"talita\", \"carol\",\n",
        "        \"alejandra\", \"ximena\", \"pauline\", \"ines\", \"in√©s\", \"beatriz\", \"raquel\",\n",
        "        \"cecilia\", \"catalina\", \"silvia\", \"ver√≥nica\", \"veronica\"\n",
        "    ]\n",
        "\n",
        "    # Feed URLs\n",
        "    LATAM_FEEDS = [\n",
        "        \"https://contxto.com/feed/\",\n",
        "        \"https://latamlist.com/feed/\"\n",
        "    ]\n",
        "\n",
        "# Allow GitHub Actions to override them:\n",
        "import os\n",
        "GOOGLE_SHEET_ID = os.getenv(\"GOOGLE_SHEET_ID\", GOOGLE_SHEET_ID)\n",
        "TZ = os.getenv(\"TZ\", TZ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "30a260b2",
      "metadata": {
        "id": "30a260b2"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# GOOGLE SHEETS INTEGRATION\n",
        "# ============================================================================\n",
        "\n",
        "class GoogleSheetsManager:\n",
        "    \"\"\"Handles Google Sheets authentication and operations\"\"\"\n",
        "\n",
        "    def __init__(self, service_account_path: str):\n",
        "        \"\"\"Initialize with service account credentials\"\"\"\n",
        "        self.service_account_path = service_account_path\n",
        "        self.client = None\n",
        "\n",
        "    def authenticate(self) -> gspread.Client:\n",
        "        \"\"\"Authenticate with Google Sheets API\"\"\"\n",
        "        if not self.client:\n",
        "            creds = ServiceAccountCredentials.from_json_keyfile_name(\n",
        "                self.service_account_path,\n",
        "                Config.SCOPE\n",
        "            )\n",
        "            self.client = gspread.authorize(creds)\n",
        "        return self.client\n",
        "\n",
        "    def open_sheet(self, sheet_id: str) -> gspread.Spreadsheet:\n",
        "        \"\"\"Open a Google Sheet by ID\"\"\"\n",
        "        client = self.authenticate()\n",
        "        return client.open_by_key(sheet_id)\n",
        "\n",
        "    def get_or_create_worksheet(\n",
        "        self,\n",
        "        sheet: gspread.Spreadsheet,\n",
        "        worksheet_name: str\n",
        "    ) -> gspread.Worksheet:\n",
        "        \"\"\"Get existing worksheet or create new one\"\"\"\n",
        "        try:\n",
        "            return sheet.worksheet(worksheet_name)\n",
        "        except gspread.WorksheetNotFound:\n",
        "            return sheet.add_worksheet(\n",
        "                title=worksheet_name,\n",
        "                rows=1000,\n",
        "                cols=20\n",
        "            )\n",
        "\n",
        "    def read_existing_urls(self, worksheet: gspread.Worksheet) -> Set[str]:\n",
        "        \"\"\"Read existing URLs from worksheet to avoid duplicates\"\"\"\n",
        "        try:\n",
        "            records = worksheet.get_all_records()\n",
        "            return {r.get(\"URL\", \"\") for r in records if r.get(\"URL\")}\n",
        "        except Exception:\n",
        "            return set()\n",
        "\n",
        "    def append_dataframe(\n",
        "        self,\n",
        "        worksheet: gspread.Worksheet,\n",
        "        df: pd.DataFrame\n",
        "    ) -> None:\n",
        "        \"\"\"Append DataFrame to worksheet\"\"\"\n",
        "        all_values = worksheet.get_all_values()\n",
        "\n",
        "        if not all_values:\n",
        "            # Empty sheet - write with headers\n",
        "            set_with_dataframe(worksheet, df)\n",
        "        else:\n",
        "            # Append without headers\n",
        "            set_with_dataframe(\n",
        "                worksheet,\n",
        "                df,\n",
        "                row=len(all_values) + 1,\n",
        "                include_column_header=False\n",
        "            )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "class DateTimeUtils:\n",
        "    \"\"\"Date and time utility functions\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def get_local_tz():\n",
        "        \"\"\"Get local timezone object\"\"\"\n",
        "        return tz.gettz(Config.TIMEZONE)\n",
        "\n",
        "    @staticmethod\n",
        "    def now() -> datetime:\n",
        "        \"\"\"Get current time in local timezone\"\"\"\n",
        "        return datetime.now(tz.tzutc()).astimezone(DateTimeUtils.get_local_tz())\n",
        "\n",
        "    @staticmethod\n",
        "    def is_within_window(dt: datetime) -> bool:\n",
        "        \"\"\"Check if datetime is within configured time window\"\"\"\n",
        "        age = DateTimeUtils.now() - dt\n",
        "        return age <= timedelta(days=Config.TIME_WINDOW_DAYS)\n",
        "\n",
        "    @staticmethod\n",
        "    def parse_feed_date(entry) -> datetime:\n",
        "        \"\"\"Parse date from feed entry\"\"\"\n",
        "        for attr in (\"published_parsed\", \"updated_parsed\"):\n",
        "            if hasattr(entry, attr) and getattr(entry, attr):\n",
        "                parsed = getattr(entry, attr)\n",
        "                dt = datetime(*parsed[:6], tzinfo=tz.tzutc())\n",
        "                return dt.astimezone(DateTimeUtils.get_local_tz())\n",
        "        return DateTimeUtils.now()\n"
      ],
      "metadata": {
        "id": "07UP9h7OxF3U"
      },
      "id": "07UP9h7OxF3U",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# TEXT ANALYSIS\n",
        "# ============================================================================\n",
        "\n",
        "class TextAnalyzer:\n",
        "    \"\"\"Analyze text for signals and entities\"\"\"\n",
        "\n",
        "    # Regex for extracting names\n",
        "    NAME_PATTERN = re.compile(\n",
        "        r\"\\b([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)\\s([A-Z√Å√â√ç√ì√ö√ë][a-z√°√©√≠√≥√∫√±]+)\\b\"\n",
        "    )\n",
        "\n",
        "    @staticmethod\n",
        "    def find_country(text: str) -> str:\n",
        "        \"\"\"Find country mentioned in text\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        for country, aliases in Config.COUNTRY_ALIASES.items():\n",
        "            for alias in aliases:\n",
        "                if alias.lower() in text_lower:\n",
        "                    return country\n",
        "        return \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def contains_terms(text: str, terms: List[str]) -> bool:\n",
        "        \"\"\"Check if text contains any of the specified terms\"\"\"\n",
        "        text_lower = text.lower()\n",
        "        return any(term.lower() in text_lower for term in terms)\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_female_founder(text: str) -> bool:\n",
        "        \"\"\"Detect if text mentions a female founder\"\"\"\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        founder_indicators = [\n",
        "            \"founded by\", \"co-founded by\", \"cofundada por\", \"fundada por\",\n",
        "            \"fundado por\", \"cofounder\", \"co-founder\", \"fundadora\",\n",
        "            \"fundador\", \"CEO\", \"CTO\"\n",
        "        ]\n",
        "\n",
        "        if not any(indicator in text_lower for indicator in founder_indicators):\n",
        "            return False\n",
        "\n",
        "        # Extract names and check against female names list\n",
        "        names = TextAnalyzer.NAME_PATTERN.findall(text)\n",
        "        for first_name, _ in names:\n",
        "            if first_name.lower() in Config.FEMALE_NAMES:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_company_name(text: str) -> str:\n",
        "        \"\"\"Attempt to extract company name from text\"\"\"\n",
        "        names = TextAnalyzer.NAME_PATTERN.findall(text)\n",
        "        if not names:\n",
        "            return \"\"\n",
        "\n",
        "        # Find most frequent name pair\n",
        "        name_pairs = [\" \".join(n) for n in names]\n",
        "        if name_pairs:\n",
        "            return max(set(name_pairs), key=name_pairs.count)\n",
        "        return \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_score(country: str, text: str) -> int:\n",
        "        \"\"\"Calculate lead score based on signals\"\"\"\n",
        "        score = 0\n",
        "\n",
        "        # Geographic relevance\n",
        "        if country:\n",
        "            score += 3\n",
        "\n",
        "        # Revenue signals\n",
        "        if TextAnalyzer.contains_terms(text, Config.POST_REVENUE_TERMS):\n",
        "            score += 3\n",
        "\n",
        "        # Female founder bonus\n",
        "        if TextAnalyzer.detect_female_founder(text):\n",
        "            score += 2\n",
        "\n",
        "        # Enterprise signals\n",
        "        if TextAnalyzer.contains_terms(text, Config.ENTERPRISE_SIGNALS):\n",
        "            score += 1\n",
        "\n",
        "        # Sector penalty\n",
        "        if TextAnalyzer.contains_terms(text, Config.SECTOR_BLACKLIST):\n",
        "            score -= 2\n",
        "\n",
        "        return max(0, min(10, score))"
      ],
      "metadata": {
        "id": "A40YW181xPmd"
      },
      "id": "A40YW181xPmd",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "b2180e89",
      "metadata": {
        "id": "b2180e89"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FEED PROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "class FeedProcessor:\n",
        "    \"\"\"Process RSS feeds and extract startup information\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def build_google_news_urls(country: str) -> List[str]:\n",
        "        \"\"\"Build Google News RSS URLs for a country\"\"\"\n",
        "        query = (\n",
        "            '(startup OR raised OR funding OR seed OR \"Series A\" '\n",
        "            'OR clients OR customers OR revenue OR facturaci√≥n OR ingresos)'\n",
        "        )\n",
        "        full_query = f'{query} {country}'\n",
        "        encoded_query = quote_plus(full_query)\n",
        "\n",
        "        return [\n",
        "            f'https://news.google.com/rss/search?q={encoded_query}&hl=es-419&gl=LA&ceid=LA:es-419',\n",
        "            f'https://news.google.com/rss/search?q={encoded_query}&hl=en&gl=US&ceid=US:en'\n",
        "        ]\n",
        "\n",
        "    @staticmethod\n",
        "    def fetch_feed_items() -> List[Dict[str, Any]]:\n",
        "        \"\"\"Fetch all feed items from configured sources\"\"\"\n",
        "        items = []\n",
        "\n",
        "        # Process Google News feeds for each country\n",
        "        for country in Config.COUNTRIES:\n",
        "            urls = FeedProcessor.build_google_news_urls(country)\n",
        "\n",
        "            for url in urls:\n",
        "                try:\n",
        "                    feed = feedparser.parse(url)\n",
        "                    items.extend(\n",
        "                        FeedProcessor._process_feed_entries(\n",
        "                            feed.entries[:Config.MAX_ITEMS_PER_FEED],\n",
        "                            \"GoogleNews\",\n",
        "                            country\n",
        "                        )\n",
        "                    )\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing feed {url}: {e}\")\n",
        "\n",
        "        # Process LatAm-specific feeds\n",
        "        for url in Config.LATAM_FEEDS:\n",
        "            try:\n",
        "                feed = feedparser.parse(url)\n",
        "                items.extend(\n",
        "                    FeedProcessor._process_feed_entries(\n",
        "                        feed.entries[:Config.MAX_ITEMS_PER_FEED],\n",
        "                        url,\n",
        "                        None\n",
        "                    )\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing feed {url}: {e}\")\n",
        "\n",
        "        return items\n",
        "\n",
        "    @staticmethod\n",
        "    def _process_feed_entries(\n",
        "        entries: List,\n",
        "        source: str,\n",
        "        default_country: Optional[str]\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"Process individual feed entries\"\"\"\n",
        "        items = []\n",
        "\n",
        "        for entry in entries:\n",
        "            dt = DateTimeUtils.parse_feed_date(entry)\n",
        "\n",
        "            if not DateTimeUtils.is_within_window(dt):\n",
        "                continue\n",
        "\n",
        "            title = entry.get(\"title\", \"\")\n",
        "            summary = entry.get(\"summary\", \"\")\n",
        "            link = entry.get(\"link\", \"\")\n",
        "\n",
        "            # Determine country\n",
        "            full_text = f\"{title}\\n{summary}\"\n",
        "            country = TextAnalyzer.find_country(full_text) or default_country or \"\"\n",
        "\n",
        "            items.append({\n",
        "                \"title\": title,\n",
        "                \"summary\": summary,\n",
        "                \"url\": link,\n",
        "                \"published\": dt,\n",
        "                \"source\": source,\n",
        "                \"country_guess\": country\n",
        "            })\n",
        "\n",
        "        return items\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# DATA TRANSFORMATION\n",
        "# ============================================================================\n",
        "\n",
        "class DataTransformer:\n",
        "    \"\"\"Transform raw feed items into structured lead data\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_html(text: str) -> str:\n",
        "        \"\"\"Remove HTML tags from text\"\"\"\n",
        "        return re.sub(r\"<[^<]+?>\", \"\", text)\n",
        "\n",
        "    @staticmethod\n",
        "    def truncate_text(text: str, max_length: int = 220) -> str:\n",
        "        \"\"\"Truncate text with ellipsis if needed\"\"\"\n",
        "        if len(text) > max_length:\n",
        "            return text[:max_length - 3] + \"‚Ä¶\"\n",
        "        return text\n",
        "\n",
        "    @staticmethod\n",
        "    def transform_items(items: List[Dict]) -> pd.DataFrame:\n",
        "        \"\"\"Transform feed items into DataFrame with scoring\"\"\"\n",
        "        rows = []\n",
        "\n",
        "        for item in items:\n",
        "            title = item[\"title\"]\n",
        "            summary = DataTransformer.clean_html(item.get(\"summary\", \"\"))\n",
        "            url = item[\"url\"]\n",
        "            country = item[\"country_guess\"]\n",
        "\n",
        "            full_text = f\"{title}. {summary}\"\n",
        "\n",
        "            # Detect signals\n",
        "            signals = []\n",
        "            if TextAnalyzer.contains_terms(full_text, Config.POST_REVENUE_TERMS):\n",
        "                signals.append(\"post-revenue\")\n",
        "            if TextAnalyzer.detect_female_founder(full_text):\n",
        "                signals.append(\"female-founder\")\n",
        "            if TextAnalyzer.contains_terms(full_text, Config.ENTERPRISE_SIGNALS):\n",
        "                signals.append(\"enterprise\")\n",
        "            if TextAnalyzer.contains_terms(full_text, Config.SECTOR_BLACKLIST):\n",
        "                signals.append(\"fintech-ish\")\n",
        "\n",
        "            # Extract company name\n",
        "            company = (\n",
        "                TextAnalyzer.extract_company_name(title) or\n",
        "                TextAnalyzer.extract_company_name(summary) or\n",
        "                \"\"\n",
        "            )\n",
        "\n",
        "            rows.append({\n",
        "                \"DateFound\": DateTimeUtils.now().strftime(\"%Y-%m-%d\"),\n",
        "                \"Company\": company,\n",
        "                \"URL\": url,\n",
        "                \"Country\": country,\n",
        "                \"Title\": title,\n",
        "                \"Snippet\": DataTransformer.truncate_text(summary),\n",
        "                \"Signals\": \", \".join(signals),\n",
        "                \"Score\": TextAnalyzer.calculate_score(country, full_text),\n",
        "                \"Source\": item[\"source\"],\n",
        "                \"Published\": item[\"published\"].strftime(\"%Y-%m-%d %H:%M\")\n",
        "            })\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "\n",
        "        if not df.empty:\n",
        "            df = df.sort_values(\n",
        "                [\"Score\", \"Published\"],\n",
        "                ascending=[False, False]\n",
        "            ).reset_index(drop=True)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "IUEGsTV8xbUM"
      },
      "id": "IUEGsTV8xbUM",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# MAIN PIPELINE\n",
        "# ============================================================================\n",
        "\n",
        "class VCSourcingPipeline:\n",
        "    \"\"\"Main pipeline orchestrator\"\"\"\n",
        "\n",
        "    def __init__(self, service_account_path: str = \"service_account.json\"):\n",
        "        \"\"\"Initialize pipeline with service account\"\"\"\n",
        "        self.sheets_manager = GoogleSheetsManager(service_account_path)\n",
        "\n",
        "    def run(self) -> Optional[pd.DataFrame]:\n",
        "        \"\"\"Run the complete sourcing pipeline\"\"\"\n",
        "        print(\"‚è≥ Starting VC Sourcing Pipeline...\")\n",
        "\n",
        "        # Step 1: Collect feed items\n",
        "        print(\"üì° Collecting feed items...\")\n",
        "        items = FeedProcessor.fetch_feed_items()\n",
        "        print(f\"‚úì Collected {len(items)} raw items\")\n",
        "\n",
        "        if not items:\n",
        "            print(\"‚ö†Ô∏è No items found within time window\")\n",
        "            return None\n",
        "\n",
        "        # Step 2: Transform data\n",
        "        print(\"üîÑ Transforming data...\")\n",
        "        df = DataTransformer.transform_items(items)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"‚ö†Ô∏è No candidate leads after transformation\")\n",
        "            return None\n",
        "\n",
        "        print(f\"‚úì Found {len(df)} candidate leads\")\n",
        "\n",
        "        # Step 3: Update Google Sheets\n",
        "        print(\"üìä Updating Google Sheets...\")\n",
        "        try:\n",
        "            sheet = self.sheets_manager.open_sheet(Config.SHEET_ID)\n",
        "            worksheet = self.sheets_manager.get_or_create_worksheet(\n",
        "                sheet,\n",
        "                Config.SHEET_NAME\n",
        "            )\n",
        "\n",
        "            # Check for duplicates\n",
        "            existing_urls = self.sheets_manager.read_existing_urls(worksheet)\n",
        "            new_df = df[~df[\"URL\"].isin(existing_urls)].copy()\n",
        "\n",
        "            if new_df.empty:\n",
        "                print(\"‚ÑπÔ∏è No new leads to add (all URLs already exist)\")\n",
        "                return df\n",
        "\n",
        "            # Append new leads\n",
        "            self.sheets_manager.append_dataframe(worksheet, new_df)\n",
        "\n",
        "            print(f\"‚úÖ Added {len(new_df)} new leads to '{Config.SHEET_NAME}'\")\n",
        "            print(f\"üîó Sheet: https://docs.google.com/spreadsheets/d/{Config.SHEET_ID}/edit\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error updating Google Sheets: {e}\")\n",
        "            return df\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "8mKiSidTxdHl"
      },
      "id": "8mKiSidTxdHl",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # For Google Colab usage:\n",
        "    # 1. Upload your service_account.json file\n",
        "    # 2. Set your Google Sheet ID in Config.SHEET_ID\n",
        "    # 3. Run the pipeline\n",
        "\n",
        "    pipeline = VCSourcingPipeline(\"service_account.json\")\n",
        "    results = pipeline.run()\n",
        "\n",
        "    if results is not None:\n",
        "        print(\"\\nüìã Top 10 Leads:\")\n",
        "        print(results.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WjfxlQFsxkXJ",
        "outputId": "dc7f44e1-17a7-4398-994a-393cb676235e"
      },
      "id": "WjfxlQFsxkXJ",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Starting VC Sourcing Pipeline...\n",
            "üì° Collecting feed items...\n",
            "‚úì Collected 151 raw items\n",
            "üîÑ Transforming data...\n",
            "‚úì Found 151 candidate leads\n",
            "üìä Updating Google Sheets...\n",
            "‚úÖ Added 151 new leads to 'Leads'\n",
            "üîó Sheet: https://docs.google.com/spreadsheets/d/1AAH4gxlPVIfQMsNkLdKEBQOJXKmtxBM_8cXzc5hi7SM/edit\n",
            "\n",
            "üìã Top 10 Leads:\n",
            "    DateFound                  Company  \\\n",
            "0  2025-08-29  Facturaci√≥n Electr√≥nica   \n",
            "1  2025-08-29          Valora Analitik   \n",
            "2  2025-08-29          Suizo Argentina   \n",
            "3  2025-08-29             El Argentino   \n",
            "4  2025-08-29             Hive Digital   \n",
            "5  2025-08-29                            \n",
            "6  2025-08-29              El Salvador   \n",
            "7  2025-08-29                            \n",
            "8  2025-08-29                Enter Top   \n",
            "9  2025-08-29      Gaming Intelligence   \n",
            "\n",
            "                                                 URL      Country  \\\n",
            "0  https://news.google.com/rss/articles/CBMi4wFBV...  El Salvador   \n",
            "1  https://news.google.com/rss/articles/CBMiwwFBV...  El Salvador   \n",
            "2  https://news.google.com/rss/articles/CBMivgFBV...     Honduras   \n",
            "3  https://news.google.com/rss/articles/CBMiywFBV...  El Salvador   \n",
            "4  https://news.google.com/rss/articles/CBMiygFBV...     Honduras   \n",
            "5  https://news.google.com/rss/articles/CBMi4wFBV...   Costa Rica   \n",
            "6  https://news.google.com/rss/articles/CBMisgFBV...   Costa Rica   \n",
            "7  https://news.google.com/rss/articles/CBMingJBV...     Honduras   \n",
            "8  https://news.google.com/rss/articles/CBMi5AFBV...     Colombia   \n",
            "9  https://news.google.com/rss/articles/CBMiuwFBV...  El Salvador   \n",
            "\n",
            "                                               Title  \\\n",
            "0  Facturaci√≥n Electr√≥nica en El Salvador: Gu√≠a s...   \n",
            "1  Empresas en Colombia podr√°n ser sancionadas po...   \n",
            "2  La Provincia confirm√≥ que Suizo Argentina es p...   \n",
            "3  Celulosa Argentina report√≥ desplome de ingreso...   \n",
            "4  Hive Digital Technologies Sets New Records in ...   \n",
            "5  El algod√≥n argentino est√° liderando la vanguar...   \n",
            "6  Ingresos tributarios de El Salvador crecen 8.1...   \n",
            "7  Canal de Panam√° prev√© acercamiento al mercado ...   \n",
            "8  Nexus International Leverages Brazil‚Äôs Project...   \n",
            "9  Brazil gambling market generates revenue of R$...   \n",
            "\n",
            "                                             Snippet  \\\n",
            "0  Facturaci√≥n Electr√≥nica en El Salvador: Gu√≠a s...   \n",
            "1  Empresas en Colombia podr√°n ser sancionadas po...   \n",
            "2  La Provincia confirm√≥ que Suizo Argentina es p...   \n",
            "3  Celulosa Argentina report√≥ desplome de ingreso...   \n",
            "4  Hive Digital Technologies Sets New Records in ...   \n",
            "5  El algod√≥n argentino est√° liderando la vanguar...   \n",
            "6  Ingresos tributarios de El Salvador crecen 8.1...   \n",
            "7  Canal de Panam√° prev√© acercamiento al mercado ...   \n",
            "8  Nexus International Leverages Brazil‚Äôs Project...   \n",
            "9  Brazil gambling market generates revenue of R$...   \n",
            "\n",
            "                    Signals  Score      Source         Published  \n",
            "0  post-revenue, enterprise      7  GoogleNews  2025-08-27 08:38  \n",
            "1  post-revenue, enterprise      7  GoogleNews  2025-08-21 16:15  \n",
            "2              post-revenue      6  GoogleNews  2025-08-28 14:38  \n",
            "3              post-revenue      6  GoogleNews  2025-08-28 13:41  \n",
            "4              post-revenue      6  GoogleNews  2025-08-28 13:08  \n",
            "5              post-revenue      6  GoogleNews  2025-08-28 11:07  \n",
            "6              post-revenue      6  GoogleNews  2025-08-28 10:58  \n",
            "7              post-revenue      6  GoogleNews  2025-08-28 06:42  \n",
            "8              post-revenue      6  GoogleNews  2025-08-28 04:07  \n",
            "9              post-revenue      6  GoogleNews  2025-08-28 01:15  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d9d29c9",
      "metadata": {
        "id": "8d9d29c9"
      },
      "source": [
        "\n",
        "### Optional: keep the key handy\n",
        "If you don't want to re-upload the JSON each session, you can mount Drive and keep `service_account.json` there.\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Then move/copy your JSON into /content/drive/MyDrive/ and change the path in authorize_from_json()\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}